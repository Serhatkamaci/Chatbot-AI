import os
import shutil
import json
from dotenv import load_dotenv
import gradio as gr
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from docx import Document
from langchain.schema import Document as LangchainDocument
from zipfile import ZipFile

load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    raise ValueError("GOOGLE_API_KEY ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ.")

# --- KÃ¼bik geÃ§miÅŸini yÃ¶netme ---
HISTORY_FILE = "kubik_history.json"


def load_kubik_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []


def save_kubik_history(history):
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def add_kubik_to_history(kubik_name):
    history = load_kubik_history()
    # EÄŸer zaten varsa, en Ã¼ste taÅŸÄ±
    history = [h for h in history if h != kubik_name]
    history.insert(0, kubik_name)
    # En fazla 20 kÃ¼bik tut
    if len(history) > 20:
        history = history[:20]
    save_kubik_history(history)
    return history


# --- DOCX dosyasÄ±nÄ± gÃ¼venli aÃ§ma ve metin alma ---
def load_docx_with_python_docx(file_path):
    try:
        doc = Document(file_path)
        paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip() != ""]
        if not paragraphs:
            print(f"DOCX dosyasÄ± iÃ§erik olarak boÅŸ: {file_path}")
            return []
        full_text = "\n".join(paragraphs)
        return [LangchainDocument(page_content=full_text, metadata={"source": file_path})]
    except Exception as e:
        print(f"DOCX dosyasÄ± aÃ§Ä±lamadÄ± ({file_path}): {e}")
        return []


# --- Belgeleri YÃ¼kle ve BÃ¶l ---
def load_and_process_documents(folder_path):
    documents = []
    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(".pdf")]
    docx_files = [f for f in os.listdir(folder_path) if f.lower().endswith(".docx")]

    if not pdf_files and not docx_files:
        print(f"'{folder_path}' klasÃ¶rÃ¼nde PDF veya DOCX dosyasÄ± bulunamadÄ±.")
        return None

    for pdf_file in pdf_files:
        file_path = os.path.join(folder_path, pdf_file)
        print(f"'{pdf_file}' yÃ¼kleniyor...")
        loader = PyPDFLoader(file_path)
        documents.extend(loader.load())

    for docx_file in docx_files:
        file_path = os.path.join(folder_path, docx_file)
        print(f"'{docx_file}' yÃ¼kleniyor...")
        documents.extend(load_docx_with_python_docx(file_path))

    if not documents:
        return None

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=300,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    return text_splitter.split_documents(documents)


# --- VektÃ¶r VeritabanÄ± Yarat veya YÃ¼kle ---
def create_or_load_vector_store(docs, index_path):
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=google_api_key
    )

    if os.path.exists(index_path):
        print("VektÃ¶r veritabanÄ± yÃ¼kleniyor...")
        return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)

    if not docs:
        print("VektÃ¶r veritabanÄ± oluÅŸturmak iÃ§in belge yok.")
        return None

    print("Yeni vektÃ¶r veritabanÄ± oluÅŸturuluyor...")
    vector_store = FAISS.from_documents(docs, embeddings)
    vector_store.save_local(index_path)
    return vector_store


# --- Global deÄŸiÅŸkenler ---
retriever = None
llm = None
current_kubik_name = ""


# --- Stream Response ---
def stream_response(message, history):
    if not message:
        yield "LÃ¼tfen bir soru girin."
        return

    global retriever, llm

    if not retriever or not llm:
        yield "Chatbot hazÄ±r deÄŸil. LÃ¼tfen Ã¶nce dosyalarÄ± yÃ¼kleyip chatbot oluÅŸturun."
        return

    docs = retriever.invoke(message)
    knowledge = "\n\n".join([doc.page_content for doc in docs])

    rag_prompt = f"""
Sen bir belge analiz uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki bilgiye dayanarak soruyu yanÄ±tla.
Sadece verilen bilgiye dayanarak yanÄ±t ver. Bilgi yoksa "ÃœzgÃ¼nÃ¼m, bu sorunun cevabÄ±nÄ± belgelerde bulamadÄ±m." de.

--- SORU ---
{message}

--- GEÃ‡MÄ°Å ---
{history}

--- BÄ°LGÄ° ---
{knowledge}
"""

    partial_message = ""
    for response in llm.stream(rag_prompt):
        partial_message += response.content
        yield partial_message


# --- Mevcut kÃ¼biÄŸi yÃ¼kle ---
def load_existing_kubik(kubik_name):
    global retriever, llm, current_kubik_name

    if not kubik_name:
        return "KÃ¼bik adÄ± boÅŸ.", False

    folder_path = os.path.join("data", kubik_name)
    index_path = os.path.join(folder_path, "faiss_index")

    if not os.path.exists(folder_path):
        return f"'{kubik_name}' kÃ¼bik klasÃ¶rÃ¼ bulunamadÄ±.", False

    if not os.path.exists(index_path):
        return f"'{kubik_name}' kÃ¼bik vektÃ¶r veritabanÄ± bulunamadÄ±.", False

    try:
        # VektÃ¶r veritabanÄ±nÄ± yÃ¼kle
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=google_api_key
        )
        vector_store = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
        retriever = vector_store.as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 10})

        # LLM'yi baÅŸlat
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash-latest",
            google_api_key=google_api_key,
            temperature=0.5,
        )

        current_kubik_name = kubik_name
        return f"'{kubik_name}' kÃ¼bik baÅŸarÄ±yla yÃ¼klendi!", True

    except Exception as e:
        return f"'{kubik_name}' kÃ¼bik yÃ¼klenirken hata: {str(e)}", False


# --- DosyalarÄ± kaydet ve chatbot hazÄ±rla ---
def save_files_and_prepare_chat(kubik_name, files):
    global current_kubik_name

    if not kubik_name:
        return "LÃ¼tfen geÃ§erli bir KÃ¼bik AdÄ± girin.", False, []

    if not files:
        return "LÃ¼tfen en az bir dosya yÃ¼kleyin.", False, []

    folder_path = os.path.join("data", kubik_name)
    os.makedirs(folder_path, exist_ok=True)

    for file in files:
        # Gradio'da yÃ¼klenen dosya objesi
        if hasattr(file, 'name') and file.name:
            # file.name dosya yolunu iÃ§erir
            original_filename = os.path.basename(file.name)
            file_path = file.name
        else:
            return "Dosya bilgisi alÄ±namadÄ±.", False, []

        # Sadece pdf ve docx kabul et
        if not (original_filename.lower().endswith(".pdf") or original_filename.lower().endswith(".docx")):
            return f"Sadece PDF ve DOCX dosyalarÄ± kabul edilir: {original_filename}", False, []

        save_path = os.path.join(folder_path, original_filename)

        # DosyayÄ± kopyala (Gradio geÃ§ici dosyasÄ±ndan hedef konuma)
        try:
            shutil.copy2(file_path, save_path)
            file_size = os.path.getsize(save_path)
            print(f"Dosya kopyalandÄ±: {save_path}, boyut: {file_size} bytes")

            # Dosya boyutunu kontrol et
            if file_size < 1000:  # 1KB'den kÃ¼Ã§Ã¼kse sorun var
                print(f"UYARI: Dosya Ã§ok kÃ¼Ã§Ã¼k gÃ¶rÃ¼nÃ¼yor: {original_filename} ({file_size} bytes)")

        except Exception as e:
            return f"Dosya kopyalanÄ±rken hata: {e}", False, []

    # Belgeleri yÃ¼kle
    docs = load_and_process_documents(folder_path)
    if not docs:
        return f"'{kubik_name}' klasÃ¶rÃ¼nde uygun dosya bulunamadÄ± veya dosyalar bozuk.", False, []

    index_path = os.path.join(folder_path, "faiss_index")

    global retriever, llm
    vector_store = create_or_load_vector_store(docs, index_path)
    if not vector_store:
        return "VektÃ¶r veritabanÄ± oluÅŸturulamadÄ±.", False, []

    retriever = vector_store.as_retriever(search_type="mmr", search_kwargs={"k": 5, "fetch_k": 10})

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        google_api_key=google_api_key,
        temperature=0.5,
    )

    current_kubik_name = kubik_name

    # KÃ¼bik geÃ§miÅŸine ekle
    history = add_kubik_to_history(kubik_name)

    return f"'{kubik_name}' chatbot'u baÅŸarÄ±yla oluÅŸturuldu! ArtÄ±k sorularÄ±nÄ±zÄ± sorabilirsiniz.", True, history


# --- Sidebar toggle fonksiyonu ---
def toggle_sidebar(current_visible):
    return gr.update(visible=not current_visible)


# --- Gradio UI ---
with gr.Blocks(title="KÃ¼bik Chatbot", css="""
.sidebar-button {
    position: fixed !important;
    top: 20px !important;
    left: 20px !important;
    z-index: 1000 !important;
    border-radius: 50% !important;
    width: 50px !important;
    height: 50px !important;
    padding: 0 !important;
}
.sidebar-container {
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 300px !important;
    height: 100vh !important;
    background: white !important;
    border-right: 1px solid #ddd !important;
    z-index: 999 !important;
    overflow-y: auto !important;
    padding: 70px 15px 15px 15px !important;
    box-shadow: 2px 0 5px rgba(0,0,0,0.1) !important;
}
.main-content {
    margin-left: 0px !important;
    transition: margin-left 0.3s ease !important;
}
.main-content.sidebar-open {
    margin-left: 300px !important;
}
""") as demo:
    sidebar_visible = gr.State(value=False)  # <- GÃ–RÃœNÃœRLÃœK DURUMU TUTULUYOR

    sidebar_toggle = gr.Button("â˜°", elem_classes="sidebar-button", variant="secondary")

    with gr.Column(visible=False, elem_classes="sidebar-container") as sidebar:
        ...

    sidebar_toggle.click(
        toggle_sidebar,
        inputs=[sidebar_visible],
        outputs=[sidebar, sidebar_visible]
    )

    # Sidebar toggle butonu


    # Sidebar paneli
    with gr.Column(visible=False, elem_classes="sidebar-container") as sidebar:
        gr.Markdown("### ğŸ“š KÃ¼bik GeÃ§miÅŸi")

        history_list = gr.Radio(
            choices=[],
            label="Mevcut KÃ¼bikler",
            interactive=True,
            value=None
        )

        load_kubik_btn = gr.Button("ğŸ“‚ KÃ¼biÄŸi YÃ¼kle", variant="secondary", size="sm")

        gr.Markdown("---")

        sidebar_new_btn = gr.Button(
            "â• Yeni KÃ¼bik OluÅŸtur",
            variant="primary",
            size="sm"
        )

    # Ana iÃ§erik alanÄ±
    with gr.Column(elem_classes="main-content") as main_content:
        # Ana form alanlarÄ± (baÅŸlangÄ±Ã§ta gÃ¶rÃ¼nÃ¼r)
        with gr.Group(visible=True) as setup_section:
            gr.Markdown("# ğŸš€ KÃ¼bik Dosya YÃ¼kleyici ve Chatbot OluÅŸturucu")
            gr.Markdown("### Belgelerinizi yÃ¼kleyin ve Ã¶zel chatbot'unuzu oluÅŸturun")

            with gr.Row():
                kubik_name = gr.Textbox(
                    label="KÃ¼bik AdÄ± (KlasÃ¶r ismi)",
                    placeholder="Ã–rn: project1",
                    scale=2
                )

            file_upload = gr.File(
                label="ğŸ“ DosyalarÄ±nÄ±zÄ± sÃ¼rÃ¼kleyip bÄ±rakÄ±n (PDF, DOCX)",
                file_types=[".pdf", ".docx"],
                file_count="multiple",
                height=120
            )

            with gr.Row():
                create_btn = gr.Button(
                    "ğŸ¤– Chatbot OluÅŸtur",
                    variant="primary",
                    size="lg",
                    scale=1
                )

            output_message = gr.Textbox(
                label="ğŸ“‹ Durum",
                interactive=False,
                lines=2
            )

        # Chatbot arayÃ¼zÃ¼ (baÅŸlangÄ±Ã§ta gizli)
        with gr.Group(visible=False) as chat_section:
            with gr.Row():
                current_kubik_display = gr.Markdown("# ğŸ’¬ Chatbot - SorularÄ±nÄ±zÄ± Sorun")
                new_chatbot_btn = gr.Button(
                    "ğŸ”„ Yeni KÃ¼bik OluÅŸtur",
                    variant="secondary",
                    size="sm"
                )

            chatbot = gr.Chatbot(
                type="messages",
                height=650,
                show_label=False,
                container=True,
                bubble_full_width=False,
                show_copy_button=True
            )

            with gr.Row():
                user_input = gr.Textbox(
                    placeholder="ğŸ’­ SorularÄ±nÄ±zÄ± buraya yazÄ±n...",
                    lines=2,
                    label="MesajÄ±nÄ±z",
                    scale=4,
                    container=True
                )
                send_btn = gr.Button("ğŸ“¤ GÃ¶nder", variant="primary", scale=1)


    def on_create_click(kubik_name, files):
        msg, success, history = save_files_and_prepare_chat(kubik_name, files)
        if success:
            return (
                msg,  # output_message
                gr.update(visible=False),  # setup_section gizle
                gr.update(visible=True),  # chat_section gÃ¶ster
                [],  # chatbot geÃ§miÅŸini temizle
                gr.update(choices=history, value=None),  # history_list gÃ¼ncelle
                f"# ğŸ’¬ {kubik_name} - SorularÄ±nÄ±zÄ± Sorun"  # current_kubik_display
            )
        else:
            history = load_kubik_history()
            return (
                msg,  # output_message
                gr.update(visible=True),  # setup_section gÃ¶rÃ¼nÃ¼r kalsÄ±n
                gr.update(visible=False),  # chat_section gizli kalsÄ±n
                [],  # chatbot
                gr.update(choices=history, value=None),  # history_list gÃ¼ncelle
                "# ğŸ’¬ Chatbot - SorularÄ±nÄ±zÄ± Sorun"  # current_kubik_display
            )


    def reset_to_setup():
        global retriever, llm, current_kubik_name
        retriever = None
        llm = None
        current_kubik_name = ""
        history = load_kubik_history()
        return (
            gr.update(visible=True),  # setup_section gÃ¶ster
            gr.update(visible=False),  # chat_section gizle
            "",  # kubik_name temizle
            None,  # file_upload temizle
            "",  # output_message temizle
            [],  # chatbot geÃ§miÅŸi temizle
            "",  # user_input temizle
            gr.update(choices=history, value=None),  # history_list gÃ¼ncelle
            "# ğŸ’¬ Chatbot - SorularÄ±nÄ±zÄ± Sorun"  # current_kubik_display
        )


    def load_kubik_from_history(selected_kubik):
        if not selected_kubik:
            return (
                "LÃ¼tfen bir kÃ¼bik seÃ§in.",  # output_message
                gr.update(visible=True),  # setup_section
                gr.update(visible=False),  # chat_section
                [],  # chatbot
                "# ğŸ’¬ Chatbot - SorularÄ±nÄ±zÄ± Sorun"  # current_kubik_display
            )

        msg, success = load_existing_kubik(selected_kubik)
        if success:
            return (
                msg,  # output_message
                gr.update(visible=False),  # setup_section gizle
                gr.update(visible=True),  # chat_section gÃ¶ster
                [],  # chatbot geÃ§miÅŸini temizle
                f"# ğŸ’¬ {selected_kubik} - SorularÄ±nÄ±zÄ± Sorun"  # current_kubik_display
            )
        else:
            return (
                msg,  # output_message
                gr.update(visible=True),  # setup_section
                gr.update(visible=False),  # chat_section
                [],  # chatbot
                "# ğŸ’¬ Chatbot - SorularÄ±nÄ±zÄ± Sorun"  # current_kubik_display
            )


    def chatbot_interact(user_message, chat_history):
        if not user_message.strip():
            return chat_history, ""

        history_str = ""
        if chat_history:
            history_pairs = []
            for i in range(0, len(chat_history), 2):
                if i + 1 < len(chat_history):
                    user_msg = chat_history[i]['content'] if isinstance(chat_history[i], dict) else str(chat_history[i])
                    bot_msg = chat_history[i + 1]['content'] if isinstance(chat_history[i + 1], dict) else str(
                        chat_history[i + 1])
                    history_pairs.append(f"KullanÄ±cÄ±: {user_msg}\nBot: {bot_msg}")
            history_str = "\n\n".join(history_pairs)

        bot_response = ""
        new_history = chat_history + [{"role": "user", "content": user_message}]

        for partial in stream_response(user_message, history_str):
            bot_response = partial
            # Son mesajÄ± gÃ¼ncelle veya yeni bot mesajÄ± ekle
            if len(new_history) > 0 and new_history[-1].get('role') == 'assistant':
                new_history[-1]['content'] = bot_response
            else:
                new_history.append({"role": "assistant", "content": bot_response})
            yield new_history, ""


    # Component baÅŸlangÄ±Ã§ durumu
    demo.load(
        lambda: gr.update(choices=load_kubik_history(), value=None),
        outputs=[history_list]
    )

    # Event handlers
    sidebar_toggle.click(
        toggle_sidebar,
        inputs=[sidebar_visible],
        outputs=[sidebar, sidebar_visible]
    )

    create_btn.click(
        fn=on_create_click,
        inputs=[kubik_name, file_upload],
        outputs=[output_message, setup_section, chat_section, chatbot, history_list, current_kubik_display],
    )

    new_chatbot_btn.click(
        fn=reset_to_setup,
        inputs=[],
        outputs=[setup_section, chat_section, kubik_name, file_upload, output_message, chatbot, user_input,
                 history_list, current_kubik_display],
    )

    sidebar_new_btn.click(
        fn=reset_to_setup,
        inputs=[],
        outputs=[setup_section, chat_section, kubik_name, file_upload, output_message, chatbot, user_input,
                 history_list, current_kubik_display],
    )

    load_kubik_btn.click(
        fn=load_kubik_from_history,
        inputs=[history_list],
        outputs=[output_message, setup_section, chat_section, chatbot, current_kubik_display],
    )

    user_input.submit(
        chatbot_interact,
        inputs=[user_input, chatbot],
        outputs=[chatbot, user_input],
    )

    send_btn.click(
        chatbot_interact,
        inputs=[user_input, chatbot],
        outputs=[chatbot, user_input],
    )

demo.launch()